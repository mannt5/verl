\
**文档二：VERL 中 DeepSeek-V3 的配置与初始化 (训练路径：Pai-Megatron-Patch)**

**目标**: 详细说明在 `verl` 中通过 Pai-Megatron-Patch 后端配置和初始化 DeepSeek-V3 模型进行训练的过程，重点是 `hf_to_mcore_config_dpskv3` 函数的实现和模型实例化。

**关键信息来源 (@docs 参考)**：
*   `docs/plans/dsv3/config_conversion.md` (配置转换的主要指南)
*   `verl/models/mcore/config_converter.py` (`hf_to_mcore_config_dpskv3` 的实现位置)
*   `verl/models/mcore/model_initializer.py` (包含使用 `GPTModel` 的 `init_mcore_model` 函数)
*   `verl/models/mcore/registry.py` (模型注册表)
*   `third_party/pai-megatron-patch/examples/deepseek_v3/run_mcore_deepseek.sh` (目标 Megatron 参数)
*   DeepSeek V3 HuggingFace `config.json` (源 HF 参数)
*   `docs/advance/megatron_extension.rst` (Megatron 模型扩展指南)
*   `docs/plans/dsv3/weight_loading.md` (权重加载与配置转换相关)

**2.1 `hf_to_mcore_config_dpskv3` 函数在 `verl/models/mcore/config_converter.py` 中的实现规划**

*   **函数签名 (建议)**:
    ```python
    def hf_to_mcore_config_dpskv3(
        hf_config: Dict,  # 解析后的 HuggingFace config.json 内容
        mcore_args: argparse.Namespace,  # 需要被填充的 Megatron-Core args 对象
        model_cfg_from_verl: Dict  # 来自 verl 配置文件 (如 ppo_megatron_trainer.yaml) 的模型配置部分
    ) -> None:
        # 实现参数映射逻辑
        pass
    ```

*   **详细参数映射表**:

    | Megatron-Core 参数 (`mcore_args.<field>`) | 来源 (`hf_config.<field>` 或 `run_mcore_deepseek.sh` 中的 `pmc_script.<param>`) | 注释 / 转换逻辑                                                                                                                                                                                                                           |
    | :--------------------------------------- | :------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
    | `num_layers`                             | `hf_config["num_hidden_layers"]`                                                            | 直接映射 (例如 61)。                                                                                                                                                                                                                           |
    | `hidden_size`                            | `hf_config["hidden_size"]`                                                                  | 直接映射 (例如 7168)。                                                                                                                                                                                                                         |
    | `num_attention_heads`                    | `hf_config["num_attention_heads"]`                                                          | 直接映射 (例如 128)。                                                                                                                                                                                                                         |
    | `num_key_value_heads`                  | `hf_config.get("num_key_value_heads", hf_config["num_attention_heads"])`                    | 直接映射 (例如 DeepSeek V3 MHA 是 128)。若无此字段，则默认为 `num_attention_heads`。                                                                                                                                                          |
    | `ffn_hidden_size`                        | `hf_config["intermediate_size"]`                                                            | 标准 FFN 中间层大小 (例如 18432)，用于非 MoE 层（如果有）或作为 MoE FFN 的基础参考。                                                                                                                                                                |
    | `max_position_embeddings`                | `hf_config["max_position_embeddings"]`                                                      | (例如 131072)。`pmc_script` 中是 `163840`。**决策**：优先使用 `hf_config` 中的值。`verl` 配置中应可覆盖。                                                                                                                              |
    | `normalization`                          | 固定值: `"RMSNorm"`                                                                         | 根据 `pmc_script`。                                                                                                                                                                                                                           |
    | `norm_epsilon`                           | `hf_config["rms_norm_eps"]`                                                                 | 直接映射 (例如 1e-6)。                                                                                                                                                                                                                         |
    | `swiglu`                                 | `True` 如果 `hf_config["hidden_act"] == "silu"` 否则 `False`                                  | 根据 `pmc_script` (`--swiglu`)。                                                                                                                                                                                                               |
    | `padded_vocab_size`                      | 计算得出。基础: `hf_config["vocab_size"] + pmc_script.EXTRA_VOCAB_SIZE (例如 467)`              | `final_vocab = hf_config["vocab_size"] + extra_vocab`。然后使用 `megatron.core.utils.make_vocab_size_divisible_by(final_vocab, divisor)`。`divisor` 依据并行设置和硬件/内核要求 (例如 128 或 `mcore_args.tensor_model_parallel_size`)。                                                                                                  |
    | `use_rotary_position_embeddings`         | `True`                                                                                      | 根据 `hf_config.rope_type` 和 `pmc_script` 推断。                                                                                                                                                                                             |
    | `rope_theta`                             | `float(hf_config["rope_theta"])`                                                            | (例如 1000000.0)。`pmc_script` 中是 `ROPE_THETA=10000`。**决策**：优先使用 `hf_config` 的值。                                                                                                                               |
    | `rope_type`                              | `"yarn"` 如果 `hf_config["rope_type"] == "yarn"`                                            | 根据 `pmc_script`。                                                                                                                                                                                                                           |
    | `use_rope_scaling`                       | `True` 如果 `hf_config["rope_type"] == "yarn"` (因为 `pmc_script` 对 YARN 使用了缩放)             | 检查 `pmc_script` 中的 `--use-rope-scaling`。                                                                                                                                                                                                |
    | `rotary_scaling_factor`                  | `40.0` (来自 `pmc_script` 的 `SCALE_FACTOR`) 如果 `use_rope_scaling` 为 `True`。                |                                                                                                                                                                                                                                             |
    | `attention_dropout`                      | `hf_config.get("attention_dropout", 0.0)`                                                   | (例如 0.0)。                                                                                                                                                                                                                               |
    | `hidden_dropout`                         | `0.0` (来自 `pmc_script` 的默认值)                                                          | `hf_config` 未指定此项；`pmc_script` 使用 `--hidden-dropout 0.0`。                                                                                                                                                                              |
    | `untie_embeddings_and_output_weights`    | `not hf_config.get("tie_word_embeddings", False)`                                           | `pmc_script` 使用 `--untie-embeddings-and-output-weights`。                                                                                                                                                                              |
    | `disable_bias_linear`                    | `True`                                                                                      | 根据 `pmc_script`。                                                                                                                                                                                                                           |
    | `kv_channels`                            | `hf_config["v_head_dim"]`                                                                   | (例如 128)。对应 `pmc_script` 中的 `--kv-channels ${V_HEAD_DIM}`。                                                                                                                                                                        |
    | `qk_layernorm`                           | `True` (从 `hf_config.multi_latent_attention_enable` 和 `pmc_script` 的 `--qk-layernorm` 推断) |                                                                                                                                                                                                                                             |
    | `multi_latent_attention`                 | `hf_config.get("multi_latent_attention_enable", False)`                                     | `pmc_script` 使用 `--multi-latent-attention`。                                                                                                                                                                                                   |
    | **MoE 特定参数**                           |                                                                                             |                                                                                                                                                                                                                                             |
    | `num_experts`                            | `hf_config["num_experts"]`                                                                  | (例如 256)。映射到 `pmc_script` 的 `--num-experts`。                                                                                                                                                                                        |
    | `moe_router_topk`                        | `hf_config["moe_router_topk"]`                                                              | (例如 8)。映射到 `pmc_script` 的 `--moe-router-topk`。                                                                                                                                                                                         |
    | `moe_ffn_hidden_size`                    | `hf_config["moe_intermediate_size"]`                                                        | (例如 2048)。映射到 `pmc_script` 的 `--moe-ffn-hidden-size`。                                                                                                                                                                                  |
    | `moe_aux_loss_coeff`                     | `0.001` (来自 `pmc_script` 的 `--moe-aux-loss-coeff`)                                         |                                                                                                                                                                                                                                             |
    | `moe_router_load_balancing_type`         | `hf_config["moe_router_load_balancing_type"]` (例如 "aux_loss")                             | 映射到 Megatron-Core 期望的枚举/字符串 (例如 `pmc_script` 中的 `seq_aux_loss`)。                                                                                                                                                                      |
    | `moe_layer_frequency_pattern`            | `model_cfg_from_verl.get("moe_layer_frequency_pattern", None)`                              | **实施要点**：`verl` 配置中应提供此模式字符串，如 `'([0]*3+[1]*58)'`。如果未提供，可尝试基于 `hf_config.moe_layer_freq` (若为1，则多数层为MoE) 生成默认模式。Megatron-Core 可能需要一个整数频率或模式字符串。 |
    | `expert_model_parallel_size`             | `model_cfg_from_verl.get("expert_parallel_size", 1)`                                        | 来自 VERL 运行时配置，而非 HF config。`pmc_script` 使用 `${EP}`。                                                                                                                                                                            |
    | `expert_tensor_parallel_size`            | `model_cfg_from_verl.get("expert_tensor_parallel_size", 1)`                                 | 同上。`pmc_script` 使用 `${ETP}`。                                                                                                                                                                                                            |
    | `moe_token_dispatcher_type`              | `"alltoall"`                                                                                | 根据 `pmc_script`。                                                                                                                                                                                                                           |
    | `moe_grouped_gemm`                       | `True` (如果 `pmc_script` 中包含 `--moe-grouped-gemm` 且期望启用)                             | `ModelArgs` 中的布尔标志。                                                                                                                                                                                                                      |
    | `moe_router_topk_scaling_factor`         | `2.5` (来自 `pmc_script`)                                                                   |                                                                                                                                                                                                                                             |
    | `moe_shared_expert_overlap`              | `True` (如果 `pmc_script` 中包含 `--moe-shared-expert-overlap`)                               | 布尔标志。                                                                                                                                                                                                                                      |
    | `moe_router_enable_expert_bias`          | `True` (如果 `pmc_script` 中包含 `--moe-router-enable-expert-bias`)                           | 布尔标志。                                                                                                                                                                                                                                      |
    | **Tokenizer 及其他**                       |                                                                                             |                                                                                                                                                                                                                                             |
    | `patch_tokenizer_type`                   | `"DeepSeekV2Tokenizer"` (来自 `pmc_script`)                                                 | **风险点**：`hf_config` 是 V3 模型。`verl` 需确保其数据准备阶段使用的 tokenizer (HF V3) 与此 Megatron tokenizer 兼容。                                                                                                                                 |
    | `transformer_impl`                       | `"transformer_engine"` (来自 `pmc_script`)                                                    |                                                                                                                                                                                                                                             |

**2.2 DeepSeek-V3 (含 MoE) 在 `verl/models/mcore/model_initializer.py` 中的模型初始化适配**

*   **核心**: 依赖 `pai-megatron-patch` 版本的 `megatron.model.gpt_model.GPTModel` 能够通过填充好的 `mcore_args` (包含上述 MoE 参数) 正确实例化 MoE 模型。
*   **MoE 层处理**:
    *   预计 `pai-megatron-patch` 的 `ParallelTransformerLayer` 会根据 `mcore_args.num_experts > 0` 和 `mcore_args.moe_layer_frequency_pattern` (或其内部逻辑转换后的逐层标记) 来决定每一层是使用标准 FFN 还是 MoE FFN。
    *   如果 `TransformerLayerSpec` 不足以传递区分 MoE 层的信号（例如，如果 MoE 层和标准层共享同一个 `TransformerLayerSpec`，仅通过 `mcore_args` 在底层区分），则现有 `init_mcore_model` 结构可能无需大改。
    *   如果需要更明确的区分（例如，MoE 层是完全不同类型的 `transformer_layer`），则可能需要如 `docs/advance/megatron_extension.rst` 所述，引入新的 `ModelLayerSpec` 并在 `init_mcore_model` 中根据层类型选择不同的 Spec。但优先假设 `pai-megatron-patch` 已在 `GPTModel` 层面处理好此问题。
*   **MLA 处理**: 类似地，MLA 的启用和配置应通过 `mcore_args` (如 `mcore_args.multi_latent_attention` 和相关的 head_dim 参数) 传递给 `GPTModel`，并由 `pai-megatron-patch` 的 `ParallelAttention` 实现来处理。
*   **模型注册**: `verl/models/mcore/registry.py` 中的 `DEEPSEEK_V3` 条目应保持，依赖 `init_mcore_model` 能够正确处理适配后的 `mcore_args`。