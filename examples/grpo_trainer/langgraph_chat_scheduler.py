import asyncio
import heapq
import logging
import os
import re
import uuid
from typing import List

import hydra
import numpy as np
import torch
from langchain_core.messages.utils import convert_to_openai_messages
from langchain_openai import ChatOpenAI
from omegaconf import DictConfig
from tensordict import TensorDict

from verl.protocol import DataProto
from verl.workers.rollout.async_server import ChatCompletionScheduler

logger = logging.getLogger(__file__)
logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))

async def get_last_messages(stream: AsyncIterator[dict]):
    """Get the last message from a stream of messages."""
    try:
        async for message in stream:
            if message.get("role") == "assistant":
                return message["content"]
    except GraphRecursionError as e:
        logger.error(f"Error getting last message from stream: {e}")
        return None

class LangGraphChatCompletionScheduler(ChatCompletionScheduler):
    """A scheduler for handling chat completions using LangGraph.

    This class extends ChatCompletionScheduler to provide async rollouts
    using LangGraph.
    """

    def __init__(
        self,
        config: DictConfig,
        model_path: str,
        server_addresses: List[str],
        max_cache_size: int = 10000,
    ):
        super().__init__(config, model_path, server_addresses, max_cache_size)
        langgraph_config = config.langgraph
        self.graph_partial = hydra.utils.instantiate(langgraph_config.graph, _partial_=True)
        self.chat_template_kwargs: dict = hydra.utils.instantiate(
            langgraph_config.get("chat_template_kwargs", {}),
            _convert_="all",  # important for tokenizer.apply_chat_template to work
        )
        self.graph_config = langgraph_config.get("graph_config", {})

    def assign_address(self):
        address = self.weighted_addresses[0][1]
        self.weighted_addresses[0][0] += 1  # type: ignore
        heapq.heapreplace(self.weighted_addresses, self.weighted_addresses[0])
        return address

    async def generate_sequences(self, batch: DataProto, **sampling_params) -> DataProto:  # type: ignore
        kwargs = dict(
            n=self.config.n,
            max_completion_tokens=self.config.response_length,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            extra_body={
                "chat_template_kwargs": {
                    "enable_thinking": self.config.get("enable_thinking", None),
                },
                # these are from openai.client.chat.completions.create arguments in ChatCompletionScheduler code
                "include_stop_str_in_output": True,
                "stop": ["</answer>"],
            },
        )

        do_sample = batch.meta_info.get("do_sample", True)
        is_validate = batch.meta_info.get("validate", False)
        if not do_sample or is_validate:
            kwargs["n"] = 1
            kwargs["temperature"] = 0

        kwargs.update(sampling_params)

        logger.info(f"generate_sequences sampling params: {kwargs}")

        tasks = []
        batch_tool_kwargs = batch.non_tensor_batch.get("tools_kwargs", None)

        for i, conversation in enumerate(batch.non_tensor_batch["raw_prompt"]):
            graph_input = dict(messages=list(conversation))
            # Pass tool kwargs if available
            if batch_tool_kwargs is not None:
                graph_input["tools_kwargs"] = batch_tool_kwargs[i]
            # Invoke LangGraph graph (n times)
            for _ in range(kwargs["n"]):
                # Get a OpenAI server address for the rollout
                address = self.assign_address()
                # Initialize LangChain LLM
                llm_kwargs = kwargs.copy()
                llm_kwargs["n"] = 1  # n samples are generated by invoking the graph n times, we need to set n=1 here
                llm = ChatOpenAI(base_url=f"http://{address}/v1", api_key="token-abc123", model=self.model_name, **llm_kwargs)  # type: ignore
                # Initialize LangGraph graph with the rollout LLM
                graph = self.graph_partial(model=llm)
                # Add thread_id to the graph config
                graph_config = self.graph_config.copy()
                if "configurable" not in graph_config:
                    graph_config["configurable"] = {}
                graph_config["configurable"]["thread_id"] = uuid.uuid4()
                # Submit a task to the pool
                tasks.append(
                    asyncio.create_task(
                        graph.ainvoke(
                            graph_input.copy(),
                            config=graph_config,
                        )
                    )
                )

        logger.info(f"submitted {len(batch.non_tensor_batch['raw_prompt'])} datapoints to LangGraph with n={kwargs['n']}")
        completed_messages = await asyncio.gather(*tasks, return_exceptions=True)
        logger.info(f"generated {len(completed_messages)} datapoints from LangGraph")

        batch_conversations = []
        for i, raw_prompt in enumerate(batch.non_tensor_batch["raw_prompt"]):
            conversations = completed_messages[i * kwargs["n"] : (i + 1) * kwargs["n"]]
            fixed_conversations = []
            for conversation in conversations:
                if isinstance(conversation, Exception):
                    logger.error(f"Failed to generate sequences for datapoint: {raw_prompt}. Using messages without assistant responses as fallback.", exc_info=conversation)
                    messages = raw_prompt
                else:
                    messages = conversation["messages"]
                fixed_conversations.append(convert_to_openai_messages(messages))
            batch_conversations.append(fixed_conversations)
        # _postprocess assumes n>=1
        return self.postprocess(batch, batch_conversations, kwargs["n"])

    def postprocess(self, batch: DataProto, batch_conversations, n: int) -> DataProto:
        """
        Based on verl/examples/ppo_trainer/naive_ppo_trainer.py:_postprocess,
        but additionally adds tool mask + returns raw conversations (lists of messages).

        Note:
            This version is based on return_assistant_tokens_mask argument of tokenizer.apply_chat_template,
            which depends on the presence of '{% generation %}' pattern in the chat template. It might not work correctly for all the models.
            (will be indicated by 'return_assistant_tokens_mask==True but chat template does not contain `{% generation %}` keyword.' warning message)
        """
        # NOTE: consistent with batch version of generate_sequences in vllm_rollout_spmd.py
        # prompts: left pad
        # responses: right pad
        # input_ids: prompt + response
        # attention_mask: [0,0,0,0,1,1,1,1, | 1,1,1,0,0,0,0,0]
        # position_ids:   [0,0,0,0,0,1,2,3, | 4,5,6,7,8,9,10,11]

        chat_template = self.chat_template_kwargs.get("chat_template", None) or self.tokenizer.get_chat_template()
        if not re.search(r"\{\%-?\s*generation\s*-?\%\}", chat_template):  # same condition as in tokenizer.apply_chat_template: https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/tokenization_utils_base.py#L1805
            raise ValueError("LangGraphChatCompletionScheduler: '{% generation %}' pattern not found in the chat template. Please use a model with '{% generation %}' in its chat template.")

        # prompts: [prompt] from input dataset
        prompts = [self.tokenizer.apply_chat_template(prompt, add_generation_prompt=False, tokenize=True, **self.chat_template_kwargs) for prompt in batch.non_tensor_batch["raw_prompt"]]
        non_tokenized_prompts = [self.tokenizer.apply_chat_template(prompt, add_generation_prompt=False, tokenize=False, **self.chat_template_kwargs) for prompt in batch.non_tensor_batch["raw_prompt"]]

        # flatten batch_conversations if n > 1
        assert len(batch_conversations) == len(prompts)
        batch_conversations = [conversation for conversations in batch_conversations for conversation in conversations]
        assert len(batch_conversations) == len(prompts) * n, f"len(batch_conversations): {len(batch_conversations)} != len(prompts) * n: {len(prompts) * n}"

        # sequences: [prompt + response]
        sequences = [self.tokenizer.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, return_assistant_tokens_mask=True, return_dict=True, **self.chat_template_kwargs) for conversation in batch_conversations]

        # raw_responses: [response] (lists of messages)
        # responses: [response] (strings with applied chat templates)
        # tool_mask: 1 for tool response tokens, 0 for others
        responses = [
            {
                "input_ids": sequence["input_ids"][len(prompts[i // n]) :],
                "attention_mask": sequence["attention_mask"][len(prompts[i // n]) :],
                "assistant_masks": sequence["assistant_masks"][len(prompts[i // n]) :],
            }
            for i, sequence in enumerate(sequences)
        ]
        raw_responses = [(conversation["messages"] if isinstance(conversation, dict) else conversation)[len(batch.non_tensor_batch["raw_prompt"][i // n]) :] for i, conversation in enumerate(batch_conversations)]

        # pad responses to the right
        responses = {
            "input_ids": torch.nn.utils.rnn.pad_sequence([torch.tensor(response["input_ids"], dtype=torch.int64) for response in responses], batch_first=True, padding_value=self.tokenizer.pad_token_id),
            "attention_mask": torch.nn.utils.rnn.pad_sequence([torch.tensor(response["attention_mask"], dtype=torch.int64) for response in responses], batch_first=True, padding_value=0),
            "assistant_masks": torch.nn.utils.rnn.pad_sequence([1 - torch.tensor(response["assistant_masks"], dtype=torch.int64) for response in responses], batch_first=True, padding_value=0),
        }

        # pad prompts to the left
        # TODO: kinda lame to tokenize twice, but I'm not sure the processing inside tokenizer.__call__ is equivalent to simply left-padding previously tokenized prompts that we already have
        prompts = self.tokenizer(non_tokenized_prompts, return_tensors="pt", padding="longest", padding_side="left")
        if n > 1:
            prompts["input_ids"] = prompts["input_ids"].repeat_interleave(n, dim=0)
            prompts["attention_mask"] = prompts["attention_mask"].repeat_interleave(n, dim=0)

        input_ids = torch.cat([prompts["input_ids"], responses["input_ids"]], dim=1)
        attention_mask = torch.cat([prompts["attention_mask"], responses["attention_mask"]], dim=1)
        position_ids = (attention_mask.cumsum(dim=1) - 1) * attention_mask

        # create a separate loss_mask tensor that will be used for loss calculation
        # loss_mask = [prompt attention mask, response attention mask (with tokens flagged as tool responses set to 0)]
        loss_mask = responses["attention_mask"].clone()
        loss_mask[responses["assistant_masks"] == 1] = 0
        loss_mask = torch.cat([prompts["attention_mask"], loss_mask], dim=1)

        batch = TensorDict(
            {"prompts": prompts["input_ids"], "responses": responses["input_ids"], "input_ids": input_ids, "attention_mask": attention_mask, "position_ids": position_ids, "loss_mask": loss_mask},
            batch_size=len(input_ids),
        )
        non_tensor_batch = {
            "raw_responses": np.array(raw_responses, dtype=object),
        }

        return DataProto(batch=batch, non_tensor_batch=non_tensor_batch)
