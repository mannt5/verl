# PPO (Proximal Policy Optimization) 示例

这个项目包含了 PPO 算法的完整示例，帮助你理解 PPO 是如何运行的。

## 文件说明

1. **ppo_example.py** - 完整的 PPO 实现，包含训练和评估
2. **ppo_simple_example.py** - 简化版本，逐步展示 PPO 的运行流程
3. **requirements.txt** - 所需依赖

## 安装依赖

```bash
pip install -r requirements.txt
```

## 运行示例

### 1. 运行简化示例（推荐先看这个）

```bash
python ppo_simple_example.py
```

这个示例会逐步展示 PPO 的运行过程：
- 如何收集经验
- 如何计算优势函数
- 如何进行策略更新
- PPO 的裁剪机制

### 2. 运行完整训练

```bash
python ppo_example.py
```

这个示例会：
- 训练一个完整的 PPO 智能体
- 在 CartPole 环境中学习平衡杆子
- 生成训练曲线图
- 评估训练好的智能体

## PPO 算法核心概念

### 1. 算法流程

```
循环:
  1. 使用当前策略 π_old 收集一批经验
  2. 计算每个状态的价值 V(s) 和优势 A(s,a)
  3. 多次更新:
     - 计算新旧策略的比率 r = π(a|s) / π_old(a|s)
     - 使用裁剪的目标函数更新策略
     - 更新价值函数
```

### 2. 核心公式

**PPO 目标函数：**
```
L^CLIP(θ) = E[min(r_t(θ)A_t, clip(r_t(θ), 1-ε, 1+ε)A_t)]
```

其中：
- r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) 是重要性采样比率
- A_t 是优势函数
- ε 是裁剪参数（通常为 0.2）

### 3. 关键特性

- **稳定性**：通过裁剪防止策略更新过大
- **样本效率**：可以对同一批数据进行多次更新
- **简单实现**：相比 TRPO 等算法更容易实现
- **性能优秀**：在多种任务上表现出色

## 实验结果

运行完整示例后，你会看到：
- 智能体在约 100-200 轮内学会平衡杆子
- 生成的训练曲线显示学习进度
- 最终评估显示稳定的高性能

## 进一步学习

1. 尝试修改超参数（学习率、裁剪范围等）
2. 在其他 Gym 环境中测试
3. 实现 GAE (Generalized Advantage Estimation)
4. 添加更多的日志和可视化