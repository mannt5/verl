# actor_rollout_ref.ref: FSDP config same as actor. For models larger than 7B, itâ€™s recommended to turn on offload for ref by default
strategy: ${actor_rollout_ref.actor.strategy}

# whether to enable torch.compile
use_torch_compile: ${actor_rollout_ref.actor.use_torch_compile}

# [Will be deprecated, use log_prob_micro_batch_size_per_gpu]
# The batch size for one forward pass in the computation of log_prob. Global batch size.
log_prob_micro_batch_size: null

# The batch size for one forward pass in the computation of log_prob. Local batch size per GPU.
log_prob_micro_batch_size_per_gpu: null

# enable dynamic batch size (sequence packing) for log_prob computation
log_prob_use_dynamic_bsz: ${actor_rollout_ref.actor.use_dynamic_bsz}

# the max token length per GPU
log_prob_max_token_len_per_gpu: ${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}

# profiler configs
profiler:

  # Required when using verl.utils.omega_conf_to_dataclass to instantiate dataclass configs in the entrypoint
  _target_: verl.utils.profiler.ProfilerConfig

  # True for each task has its own database, False for all tasks in one training step share one database.
  discrete: False

  # Whether to profile all ranks.
  all_ranks: False

  # The ranks that will be profiled. [] or [0,1,...]
  ranks: []